# Text2SQL Submission to BirdBench

This branch contains the code to reproduce the results of our approach to the BirdBench challenge.

## Setup and Installation

To set up the codebase and run the evaluation, first clone the repository and switch to the `submission` branch:

```bash
git clone https://github.com/premAI-io/text2sql.git
git checkout -b submission
```

Next, create a new virtual environment. This can be done using `pyenv` or `conda`. If using `conda`, you can create the environment and install the requirements as follows:

```bash
conda create -n submission python=3.10
pip install -U -r requirements.txt
```

## Usage

Ensure that your dataset follows the required directory structure. For example, if you downloaded the dev dataset and stored it in the `./data/bird` folder, it should have the following structure:

```plaintext
bird
├── dev_databases
│   ├── california_schools
│       ├── california_schools.sqlite
│   ├── card_games
│   ├── codebase_community
│   ├── debit_card_specializing
│   ├── european_football_2
│   ├── financial
│   ├── formula_1
│   ├── student_club
│   ├── superhero
│   ├── thrombosis_prediction
│   └── toxicology
├── dev.json
├── dev.sql
├── dev_tables.json
└── dev_tied_append.json
```

### Dataset Structure Requirements

The dataset must include the following:

1. **`dev_databases` Folder**: This folder contains subfolders for each database, each with a corresponding `.sqlite` file matching the subfolder's name.
   
2. **`dev.json` File**: Contains metadata, including mappings of database paths, questions, filters, and other relevant information.

### Notes

For our approach, these two components are mandatory. We assume all databases are `.sqlite` files. 

If your dataset files are named differently (e.g., for `test_dataset`), you can rename them to match this structure or adjust the names when instantiating them with our API, as shown below:

```python
from text2sql.dataset import BirdDevDataset

# Define paths and model
data_path = "./path/to/test/dataset"              # e.g., ./data/bird
model_name_or_path = "premai-io/prem-1B-SQL"      # Public model

dataset = BirdDevDataset(
    data_path=data_path,
    databases_folder_name="dev_databases",  # Change if named differently
    json_file_name="dev.json",              # Change if named differently
    num_fewshot=5,                          # Do not change
    model_name_or_path=model_name_or_path,  # Do not change
)
```

### Generator

The `Generator` object of our Text2SQL API generates results. Here's how to use it:

```python
from text2sql.generator.from_hf import GeneratorHFModel

generator = GeneratorHFModel(
    model_or_name_or_path=model_name_or_path,
    experiment_name="test_pretext2sql",                 # Name of your experiment
    type="test",                                        # Do not change
    device="cuda:0",                                    # CUDA device mapping
)

responses = generator.generate_and_save_results(
    data=dataset,
    temperature=0.1,    # Do not change
    max_retries=5,      # Do not change
    force=True,         # Set to True if re-generation is needed
)
```

### Output Structure

Responses generated by the `Generator` are saved inside `text2sql/experiments/test/test_pretext2sql`, determined by the `experiment_name`. The folder will contain the following:

- **`predict.json`**: Contains details of each response run, extending the original `dev.json` (or `test.json`) file with additional information like `generated` SQL queries.

- **`predict.sql`**: A sequential list of generated SQL queries. Ideally, the SQLs should match the `generated` entries in `predict.json`.

Here is how the `predict.json` looks like:

```json
    {
        "question_id": 0,
        "db_id": "california_schools",
        "question": "What is the highest eligible free rate for K-12 students in the schools in Alameda County?",
        "evidence": "Eligible free rate for K-12 = `Free Meal Count (K-12)` / `Enrollment (K-12)`",
        "SQL": "SELECT `Free Meal Count (K-12)` / `Enrollment (K-12)` FROM frpm WHERE `County Name` = 'Alameda' ORDER BY (CAST(`Free Meal Count (K-12)` AS REAL) / `Enrollment (K-12)`) DESC LIMIT 1",
        "difficulty": "simple",
        "db_path": "data/bird/validation/dev_databases/california_schools/california_schools.sqlite",
        "prompt": "<\uff5cbegin\u2581of\u2581sentence\uff5c>You are an AI programming assistant, utilizing the ... # SQL:\n",
        "generated": "SELECT MAX(CAST(T2.`Free Meal Count (K-12)` AS REAL) / T2.`Enrollment (K-12)`) FROM schools AS T1 INNER JOIN frpm AS T2 ON T1.CDSCode = T2.CDSCode WHERE T1.County = 'Alameda'",
    },
    {
        "question_id": 1,
        "db_id": "california_schools",
        "question": "Please list the lowest three eligible free rates for students aged 5-17 in continuation schools.",
        "evidence": "Eligible free rates for students aged 5-17 = `Free Meal Count (Ages 5-17)` / `Enrollment (Ages 5-17)`",
        "SQL": "SELECT `Free Meal Count (Ages 5-17)` / `Enrollment (Ages 5-17)` FROM frpm WHERE `Educational Option Type` = 'Continuation School' AND `Free Meal Count (Ages 5-17)` / `Enrollment (Ages 5-17)` IS NOT NULL ORDER BY `Free Meal Count (Ages 5-17)` / `Enrollment (Ages 5-17)` ASC LIMIT 3",
        "difficulty": "moderate",
        "db_path": "data/bird/validation/dev_databases/california_schools/california_schools.sqlite",
        "prompt": "<\uff5cbegin\u2581of\u2581sentence\uff5c>You are an AI programming assistant, utilizing the ... for students aged 5-17 in continuation schools.\n\n# SQL:\n",
        "generated": "SELECT CAST(`Free Meal Count (Ages 5-17)` AS REAL) / `Enrollment (Ages 5-17)` FROM frpm WHERE `Educational Option Type` = 'Continuation School' ORDER BY `Free Meal Count (Ages 5-17)` / `Enrollment (Ages 5-17)` ASC LIMIT 3",
    }
```

Here is how the `predict.sql` looks like:

```SQL
SELECT MAX(CAST(T2.`Free Meal Count (K-12)` AS REAL) / T2.`Enrollment (K-12)`) FROM schools AS T1 INNER JOIN frpm AS T2 ON T1.CDSCode = T2.CDSCode WHERE T1.County = 'Alameda'
SELECT CAST(`Free Meal Count (Ages 5-17)` AS REAL) / `Enrollment (Ages 5-17)` FROM frpm WHERE `Educational Option Type` = 'Continuation School' ORDER BY `Free Meal Count (Ages 5-17)` / `Enrollment (Ages 5-17)` ASC LIMIT 3
```

> Now after this, you can use our evaluator or you can use your own to evaluate it. If the judges uses their own evaluator then it is requested to also do the same evaluation using our evaluator (just to see they are equivalent). If not then we from our side can re-iterate on future versions. 

### Evaluation

We provide a simple evaluator based on the main [BirdBench code](https://github.com/AlibabaResearch/DAMO-ConvAI/tree/main/bird). Evaluation is done iteratively without multiprocessing. Here’s how to use our evaluator:

```python
from text2sql.executor.from_sqlite import ExecutorFromSQLite

executor = ExecutorFromSQLite(
    experiment_path=generator.experiment_path,
)

# Compute accuracy and VES metrics
ex_acc = executor.compute(
    model_responses=responses,
    metric="accuracy",
    filter_by="difficulty"
)

ves = executor.compute(
    model_responses=responses,
    metric="ves",
    filter_by="difficulty"
)

print(f"Accuracy: {ex_acc}")
print(f"VES: {ves}")
```

The same script is available in [main.py](/main.py) and [example.ipynb](/example.ipynb). Adjust paths based on your dataset file location and run:

```bash
python main.py
```

This guide covers loading datasets, generating predictions, and evaluating results. For questions, contact: anindyadeep@premai.io

