{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating Text-to-SQL Performance on BirdBench Test Dataset\n",
    "\n",
    "This notebook demonstrates how to evaluate Prem AI's text-to-SQL approach using a test or development dataset from BirdBench. To begin, ensure that your dataset follows the required directory structure:\n",
    "\n",
    "```python\n",
    "├── dev_databases\n",
    "│   ├── california_schools\n",
    "│       ├── california_schools.sqlite\n",
    "│   ├── card_games\n",
    "│   ├── codebase_community\n",
    "│   ├── debit_card_specializing\n",
    "│   ├── european_football_2\n",
    "│   ├── financial\n",
    "│   ├── formula_1\n",
    "│   ├── student_club\n",
    "│   ├── superhero\n",
    "│   ├── thrombosis_prediction\n",
    "│   └── toxicology\n",
    "├── dev.json\n",
    "├── dev.sql\n",
    "├── dev_tables.json\n",
    "└── dev_tied_append.json\n",
    "```\n",
    "\n",
    "In this example, we use the BirdBench development dataset. The dataset structure should include the following mandatory components:\n",
    "\n",
    "1. **`dev_databases` Folder**: This directory contains subfolders named after each database, each containing a corresponding `.sqlite` file. The `.sqlite` file must match the subfolder's name exactly.\n",
    "\n",
    "2. **`dev.json` File**: This file contains metadata, including mappings of database paths, questions, filters, and other relevant information.\n",
    "\n",
    "If your dataset files are named differently (e.g., for test datasets), you can either rename them to match this structure or adjust the names while you instantiate them with our API. Here is an example on how to do that. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from text2sql.dataset import BirdDevDataset\n",
    "\n",
    "# inside that it should have the json file and dev_databases folder\n",
    "data_path = \"./path/to/test/dataset\"\n",
    "model_name_or_path = \"premai-io/prem-1B-SQL\"\n",
    "\n",
    "dataset = BirdDevDataset(\n",
    "    data_path=data_path,\n",
    "    databases_folder_name=\"dev_databases\",  # Change the name here if there is other name\n",
    "    json_file_name=\"dev.json\",              # Change the name here if the name is different\n",
    "    num_fewshot=5,                          # This is not to be changed\n",
    "    model_name_or_path=model_name_or_path,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So to summarize the only thing which is rigid is the structure of the data, i.e.\n",
    "\n",
    "- A parent folder containing folders representing different database.\n",
    "- Inside each database folder, a .sqlite file with the same name as the database folder name.\n",
    "- A .json file which has all the information about the different DBs, question etc.\n",
    "\n",
    "Now let's move into the `Generator` section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generator\n",
    "\n",
    "A generator object of our text2sql API helps to generate the results. This is how we use it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from text2sql.generator.from_hf import GeneratorHFModel\n",
    "\n",
    "generator = GeneratorHFModel(\n",
    "    model_or_name_or_path=model_name_or_path,\n",
    "    experiment_name=\"test_pretext2sql\",                 # Give whatever name you want\n",
    "    type=\"test\",                                        # This should not change\n",
    "    device=\"cuda:0\",                                    # Cuda device mapping\n",
    ")\n",
    "\n",
    "responses = generator.generate_and_save_results(\n",
    "    data=dataset,\n",
    "    temperature=0.1,    # This is not to be changed\n",
    "    max_retries=5       # This is not to be changed\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this testing, we are going to use our official model and the `max_retries` and `temperature` parameters are not subjected to change. Once all the responses are generated, a folder named `experiment` is created under the folder in which the program is run. So in this example, a folder named: `text2sql/experiments/test/test_pretext2sql` will be created. Inside this you will find a `predict.json` file which will contain the following information (each blob of the JSON):\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"question_id\": 0,\n",
    "    \"db_id\": \"california_schools\",\n",
    "    \"question\": \"What is the highest eligible free rate for K-12 students in the schools in Alameda County?\",\n",
    "    \"evidence\": \"Eligible free rate for K-12 = `Free Meal Count (K-12)` / `Enrollment (K-12)`\",\n",
    "    \"SQL\": \"SELECT `Free Meal Count (K-12)` / `Enrollment (K-12)` FROM frpm WHERE `County Name` = 'Alameda' ORDER BY (CAST(`Free Meal Count (K-12)` AS REAL) / `Enrollment (K-12)`) DESC LIMIT 1\",\n",
    "    \"difficulty\": \"simple\",\n",
    "    \"db_path\": \"/root/anindya/text2sql/data/bird/validation/dev_databases/california_schools/california_schools.sqlite\",\n",
    "    \"prompt\": \"<\\uff5cbegin\\u2581of\\u2581sentence\\uff5c>You ... students in the schools in Alameda County?\\n\\n# SQL:\\n\",\n",
    "    \"generated\": \"SELECT MAX(CAST(T1.`Free Meal Count (K-12)` AS REAL) / T1.`Enrollment (K-12)`) FROM frpm AS T1 INNER JOIN schools AS T2 ON T1.CDSCode = T2.CDSCode WHERE T2.County = 'Alameda'\",\n",
    "},\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluator\n",
    "\n",
    "This is the simple evaluator whose logic has been taken from the main [BirdBench code](https://github.com/AlibabaResearch/DAMO-ConvAI/tree/main/bird). However there is no multiprocessing applied, so evaluation is been done iteratively. If you want to use your own evaluator then you can use the information from `generated` key of the each blob inside the `predict.json` file which is saved after the generation is complete. It also contains all the additional information which was there in the dataset.\n",
    "\n",
    "Here is the code on how to evaluate using prem text2sql API. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from text2sql.executor.from_sqlite import ExecutorFromSQLite\n",
    "\n",
    "executor = ExecutorFromSQLite(\n",
    "    experiment_path=generator.experiment_path,\n",
    ")\n",
    "\n",
    "ex_acc = executor.compute(\n",
    "    model_responses=responses,\n",
    "    metric=\"accuracy\",\n",
    "    filter_by=\"difficulty\"\n",
    ")\n",
    "\n",
    "ves = executor.compute(\n",
    "    model_responses=responses,\n",
    "    metric=\"ves\",\n",
    "    filter_by=\"difficulty\"\n",
    ")\n",
    "\n",
    "print(f\"Accuracy: {ex_acc}\")\n",
    "print(f\"VES: {ves}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is how we load dataset, generate predictions and evaluate the generations. For any kind of questions please reach us to: anindyadeep@premai.io"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
